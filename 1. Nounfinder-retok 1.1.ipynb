{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c244475-6b73-4f07-b91c-30049e8e7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importAsDF(files, de):\n",
    "    \"\"\"\n",
    "    This Method imports a filelist of txt documents as dataframe\n",
    "    @param files: a list of files in txt format\n",
    "    @param de: boolean, if set True the language of the files is set german otherwise english\n",
    "    @return: dataframe that enlists the txtdocuments content sorted by job, company, job description, language\n",
    "    \"\"\"\n",
    "    startdfs = [0 for x in range(len(files))]\n",
    "\n",
    "    #import txt files as dataframes\n",
    "    for i in range(len(files)):\n",
    "        df= pd.read_csv(files[i], delimiter = \"\\t\", header = None)\n",
    "        startdfs[i]=df\n",
    "    #create a more sorted df\n",
    "    lang_dict = {}\n",
    "    job_dict = {}\n",
    "    company_dict = {}\n",
    "    descr_dict = {}\n",
    "\n",
    "    for j in range(len(startdfs)):\n",
    "        #current df\n",
    "        df = startdfs[j]\n",
    "        #set language\n",
    "        if de:\n",
    "            lang =\"de\"\n",
    "        else:\n",
    "            lang= \"en\"\n",
    "        lang_dict[j]= lang;\n",
    "        #set job\n",
    "        job = df.iloc[0,0]\n",
    "        job_dict[j]= job\n",
    "        #set company\n",
    "        company = df.iloc[1,0]\n",
    "        company_dict[j]= company;\n",
    "        #set description - rest of the dataframe\n",
    "        descr = \"\"\n",
    "        for i in range(2,len(df.index)):\n",
    "            descr = descr + \" \"+ df.iloc[i,0].strip(\"◾\")\n",
    "        descr_dict[j] = descr\n",
    "    #convert dicts to series\n",
    "    lang_series = pd.Series(lang_dict)\n",
    "    job_series = pd.Series(job_dict)\n",
    "    company_series = pd.Series(company_dict)\n",
    "    descr_series = pd.Series(descr_dict)\n",
    "    #convert series to df\n",
    "    df = pd.DataFrame({'job':job_series, 'company':company_series, 'description':descr_series, 'language':lang_series,})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb6daf6-21cf-4c36-9b6f-d446998d9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getjobdescriptions(df):\n",
    "    \"\"\"\n",
    "    This Method puts all jobdescriptions of a df into one string\n",
    "    @param df: The Job dataframe from which to take the descriptions from\n",
    "    @return: string filled with all job descriptions at once\n",
    "    \"\"\"\n",
    "    #get all descriptions\n",
    "    descrstr = \"\"\n",
    "    for i in range(len(df)):\n",
    "        descrstr =descrstr +\" \"+ df.iloc[i,2]\n",
    "    return (removespecials(descrstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7d5e193-f1ec-45b3-9539-694b6c9e9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getjobdescriptionsaslist(df):\n",
    "    \"\"\"\n",
    "    This Method puts all jobdescriptions of a df into one list of strings\n",
    "    @param df: The Job dataframe from which to take the descriptions from\n",
    "    @return: list of strings filled with all job descriptions doc by doc\n",
    "    \"\"\"\n",
    "    #get all descriptions\n",
    "    descrstr = []\n",
    "    for i in range(len(df)):\n",
    "        newstr = removespecials(df.iloc[i,2].lower())\n",
    "        descrstr += [newstr]\n",
    "    return descrstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd3db15-319c-44f9-b1a3-2d2764bd14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removespecials(str):\n",
    "    \"\"\"\n",
    "    This Method removes specialcaracters\n",
    "    @param str: the string from whom those specialcharacters shall be removed\n",
    "    @return: string without those specialcaracters\n",
    "    \"\"\"\n",
    "    return str.replace(\"◾\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"•\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c2d158-19cc-4688-be4c-43f9f2bb5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def firstspacy(descrstr, de):\n",
    "    \"\"\"\n",
    "    This Method takes a string and returns a dataframe with counted occurence of propernouns. It detects compound nouns and counts them as those compound nouns but not as single nouns\n",
    "    @param str: the string win which to found nouns\n",
    "    @param de:  boolean, if set True the language of the files is set german otherwise english\n",
    "    @return: dataframe of single proper nouns and compound proper nouns and their count\n",
    "    \"\"\"\n",
    "    #let the spacy magic begin ^^\n",
    "    import spacy\n",
    "    from spacy.tokens import Doc\n",
    "    from spacy.lang.de import German\n",
    "    from spacy.lang.en import English\n",
    "    \n",
    "    if(de):\n",
    "        nlp = spacy.load(\"de_core_news_sm\")\n",
    "    else:\n",
    "        nlp = spacy.load(\"en_core_web_sm\") \n",
    "    \n",
    "    nlp.max_length = 2000000\n",
    "    \n",
    "    nlp.add_pipe(\"merge_entities\")\n",
    "    \n",
    "    doc = nlp(descrstr)\n",
    "\n",
    "    #Matching\n",
    "    from spacy.matcher import Matcher\n",
    "\n",
    "    #get compound proper nouns\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
    "    pattern2 = [{\"POS\": \"PROPN\"}]\n",
    "    matcher.add(\"pat\", [pattern])\n",
    "    matcher.add(\"pat\", [pattern2])\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    results =  [doc[start:end].text for match_id, start, end in matches]\n",
    "    \n",
    "    #put compound proper nouns nouns in dict and count them\n",
    "    pnounscount = {}\n",
    "    pnouns = {}\n",
    "    index = []\n",
    "    for result in results:\n",
    "            if result in pnounscount:\n",
    "                pnounscount[result] = pnounscount[result]+1\n",
    "            else:\n",
    "                pnounscount[result] = 1\n",
    "                pnouns[result] = result\n",
    "                index +=[result]\n",
    "    \n",
    "    #convert to series and dataframe, sort by count column descending\n",
    "    pncount = pd.Series(pnounscount)\n",
    "    pn = pd.Series(pnouns)\n",
    "    df = pd.DataFrame({'count':pncount, 'term':pn}, index = index)\n",
    "    df = df.sort_values(by = 'count', ascending = False)\n",
    "    \n",
    "    \"\"\"\n",
    "    #Is skill Erweiterung per whitelist - TODO: Ausbauen\n",
    "    from spacy.tokens import Token\n",
    "\n",
    "    is_skill = lambda token: token.text in ['Java', 'C#', 'Python','C']\n",
    "    Token.set_extension('skill', getter = is_skill, force = True)\n",
    "    doc = nlp('Java, C# and Python are programming languages')\n",
    "\n",
    "    print ([(token.text , token._.skill) for token in doc])\n",
    "    \"\"\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c001f092-bdbb-4484-9ac6-758a2a7965b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(df):\n",
    "    \"\"\"\n",
    "    This Method plots a dataframe of proper nouns and their counts in a bar diagram\n",
    "    @param df: dataframe to plot\n",
    "    \"\"\" \n",
    "    #Plot \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    plt.ylabel('Anzahl der Nennungen')\n",
    "    plt.xlabel('Begriff')\n",
    "    df['count'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073520dc-2dc2-46a3-93cb-26295aa2a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDF(files, de):\n",
    "    \"\"\"\n",
    "    This Method uses other methods to take a list of txt files and convert it into a dataframe with proper nouns and their count within those strings\n",
    "    @param files: a filelist of txt files\n",
    "    @param de: boolean, true -> german , false -> english\n",
    "    @return: dataframe that contains propernouns and their count\n",
    "    \"\"\"\n",
    "    df1 = importAsDF(files,de)\n",
    "    df1.to_csv(r\"C:\\Users\\thegy\\Documents\\Studium\\HS FL 4\\Mining\\DFs\\jobdf.csv\", sep=\";\",decimal=\",\", index=False)\n",
    "    descr = getjobdescriptions(df1)\n",
    "    #descr String nicht lower, replace 3 hauptjob in Beschreibungsstrings\n",
    "    df = firstspacy(descr, de)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdab51dc-b008-4298-9a44-a0732ef60d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropstopwords(df):\n",
    "    \n",
    "    #get all companys\n",
    "    df11 = importAsDF(files,True)\n",
    "    comp = \"\"\n",
    "    for i in range(len(df11)):    \n",
    "        comp = comp +\" \"+ (df11.iloc[i,1])\n",
    "        \n",
    "    stopwords = comp.split(\" \")\n",
    "    stopwords += ['d','Data Engineer','Data Analyst','Data Scientist', 'and', '€', 'EUR', 'English','Frankfurt','Main','UK']\n",
    "    \n",
    "    for i in range(len(stopwords)):\n",
    "        if stopwords[i] in df.index:\n",
    "            df = df.drop(stopwords[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703c460-1e93-4f80-8a8c-129016a27750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read txt files, count proper nouns (compound proper nouns as well and those compounds explicitly not as single nouns)\n",
    "Get Dataframe of those nouns and ther count and plot them\n",
    "\"\"\"\n",
    "\n",
    "#find files\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "files = glob.glob(r\"C:\\Users\\thegy\\Documents\\Studium\\HS FL 4\\Mining\\Jobs DE\\*.txt\")\n",
    "df = getDF(files, True)\n",
    "\n",
    "filesen = glob.glob(r\"C:\\Users\\thegy\\Documents\\Studium\\HS FL 4\\Mining\\Jobs EN\\*.txt\")\n",
    "dfen = getDF(filesen, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b21ca2-820e-46f9-856f-a129e1384967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dropstopwords(df)\n",
    "dfen = dropstopwords(dfen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cea0a3-6814-4442-8f6b-54ba819129b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filecount de: ',len(files))\n",
    "print('Filecount en: ',len(filesen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62023825-f585-459b-b0c3-7475f2676694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(df[:20])\n",
    "plot(dfen[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af331a04-09c8-485a-80dc-fac435967fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\thegy\\Documents\\Studium\\HS FL 4\\Mining\\DFs\\nouns_de_retok.csv\", sep=\";\",decimal=\",\", index=False)\n",
    "dfen.to_csv(r\"C:\\Users\\thegy\\Documents\\Studium\\HS FL 4\\Mining\\DFs\\nouns_en_retok.csv\", sep=\";\",decimal=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44e144-7e63-4e4c-8581-8b5cb8c691f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
